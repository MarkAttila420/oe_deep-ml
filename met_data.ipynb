{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cél: Naponta bontott időjárási adatok letöltése több európai fővárosra 2020 és 2024 között.\n",
    "A letöltött adatokat egyesítjük, hogy egy közös DataFrame-ben legyenek.\n",
    "Ez szolgál majd bemenetként a mélytanuló modellhez, amely Budapest jövőbeli hőmérsékletét próbálja majd becsülni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install meteostat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modellparaméterek\n",
    "HIDDEN_SIZE = 32          # LSTM rejtett réteg mérete\n",
    "NUM_LAYERS = 1            # LSTM rétegek száma\n",
    "LEARNING_RATE = 0.001    # Tanulási ráta\n",
    "BATCH_SIZE = 128           # Batch méret\n",
    "N_STEPS = 7               # Rolling window méret (időlépések)\n",
    "EARLY_STOPPING_PATIENCE = 10\n",
    "TARGET_CITY = \"Budapest\"\n",
    "MAX_RADIUS_KM = 2000\n",
    "ONLY_CAPITALS = True\n",
    "RELOAD_CITIES = False\n",
    "CACHE_VERIFIED = \"verified_coordinates.pkl\"\n",
    "CACHE_FAILED = \"unresolvable_cities.pkl\"\n",
    "\n",
    "# Célváltozó\n",
    "TARGET_COLUMN = \"Budapest_tmax\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# Numerikus, adatelemzés\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Földrajzi távolságok\n",
    "from geopy.distance import geodesic\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# Időjárás adatok\n",
    "from meteostat import Point, Daily\n",
    "\n",
    "# Deep learning - PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# SimpleMaps CSV beolvasása\n",
    "cities_df = pd.read_csv(\"worldcities.csv\")  # Ha máshogy nevezted el, cseréld a fájlnevet\n",
    "\n",
    "# A célváros kiválasztása\n",
    "target_row = cities_df[cities_df[\"city_ascii\"] == TARGET_CITY]\n",
    "if target_row.empty:\n",
    "    raise ValueError(f\"A megadott célváros ({TARGET_CITY}) nem található az adatbázisban.\")\n",
    "\n",
    "target_latlon = (target_row.iloc[0][\"lat\"], target_row.iloc[0][\"lng\"])\n",
    "\n",
    "# Szűrés csak országfővárosokra, ha szükséges\n",
    "if ONLY_CAPITALS:\n",
    "    filtered_df = cities_df[cities_df[\"capital\"] == \"primary\"].copy()\n",
    "else:\n",
    "    filtered_df = cities_df.copy()\n",
    "\n",
    "# Távolság szerinti szűrés\n",
    "def within_radius(row):\n",
    "    return geodesic(target_latlon, (row[\"lat\"], row[\"lng\"])).km <= MAX_RADIUS_KM\n",
    "\n",
    "filtered_df = filtered_df[filtered_df.apply(within_radius, axis=1)]\n",
    "\n",
    "# Célvárost biztosan hozzáadjuk, ha még nem szerepel\n",
    "if TARGET_CITY not in filtered_df[\"city_ascii\"].values:\n",
    "    filtered_df = pd.concat([filtered_df, target_row])\n",
    "\n",
    "# Koordinátaszótár előállítása\n",
    "city_coordinates = {\n",
    "    row[\"city_ascii\"]: (row[\"lat\"], row[\"lng\"]) for _, row in filtered_df.iterrows()\n",
    "}\n",
    "\n",
    "print(f\"{len(city_coordinates)} város kiválasztva {MAX_RADIUS_KM} km-en belül {TARGET_CITY} körül.\")\n",
    "print(\" ->\", \", \".join(sorted(city_coordinates.keys())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Beállítások ====\n",
    "\n",
    "\n",
    "# === Korábbi koordináták betöltése (ha van) ===\n",
    "if os.path.exists(CACHE_VERIFIED):\n",
    "    with open(CACHE_VERIFIED, \"rb\") as f:\n",
    "        verified_coordinates = pickle.load(f)\n",
    "    print(f\"{len(verified_coordinates)} város koordinátája már korábban letöltve.\")\n",
    "else:\n",
    "    verified_coordinates = {}\n",
    "\n",
    "# === Korábban nem található városok betöltése (ha nem akarunk újra próbálkozni) ===\n",
    "if os.path.exists(CACHE_FAILED) and not RELOAD_CITIES:\n",
    "    with open(CACHE_FAILED, \"rb\") as f:\n",
    "        unresolvable_cities = pickle.load(f)\n",
    "else:\n",
    "    unresolvable_cities = set()\n",
    "\n",
    "# === Geolokátor inicializálása ===\n",
    "geolocator = Nominatim(user_agent=\"weather_project\")\n",
    "\n",
    "# === Lekérdezés csak a szükséges városokra ===\n",
    "for city in city_coordinates.keys():\n",
    "    if city in verified_coordinates:\n",
    "        continue  # már lekérdezve\n",
    "    if not RELOAD_CITIES and city in unresolvable_cities:\n",
    "        print(f\"{city}: korábban nem található, most sem próbáljuk újra.\")\n",
    "        continue\n",
    "    try:\n",
    "        location = geolocator.geocode(city + \", Europe\")\n",
    "        if location:\n",
    "            lat, lon = location.latitude, location.longitude\n",
    "            verified_coordinates[city] = (lat, lon)\n",
    "            print(f\"{city}: ({lat:.4f}, {lon:.4f})\")\n",
    "        else:\n",
    "            print(f\"{city}: Nem található.\")\n",
    "            unresolvable_cities.add(city)\n",
    "    except Exception as e:\n",
    "        print(f\"Hiba {city} lekérdezésekor: {e}\")\n",
    "        unresolvable_cities.add(city)\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "# === Cache mentése ===\n",
    "with open(CACHE_VERIFIED, \"wb\") as f:\n",
    "    pickle.dump(verified_coordinates, f)\n",
    "with open(CACHE_FAILED, \"wb\") as f:\n",
    "    pickle.dump(unresolvable_cities, f)\n",
    "\n",
    "# === Végeredmény beállítása ===\n",
    "city_coordinates = verified_coordinates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime(2000, 1, 1)\n",
    "end = datetime(2024, 12, 31)\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "all_data = {}\n",
    "\n",
    "for name, (lat, lon) in city_coordinates.items():\n",
    "    file_path = f\"data/{name}.csv\"\n",
    "    needs_download = True\n",
    "    data = None\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            data = pd.read_csv(file_path, parse_dates=[\"time\"], index_col=\"time\")\n",
    "            # Ellenőrizzük, hogy az adatok lefedik-e a kívánt időintervallumot\n",
    "            if not data.empty:\n",
    "                min_date, max_date = data.index.min(), data.index.max()\n",
    "                if min_date <= start and max_date >= end:\n",
    "                    needs_download = False\n",
    "                    print(f\"{name}: meglévő adat teljes időtartományban.\")\n",
    "                else:\n",
    "                    print(f\"{name}: adat hiányos (elérhető: {min_date.date()} – {max_date.date()}).\")\n",
    "        except Exception as e:\n",
    "            print(f\"{name}: hiba a fájl beolvasásakor, új letöltés. ({e})\")\n",
    "\n",
    "    if needs_download:\n",
    "        try:\n",
    "            point = Point(lat, lon)\n",
    "            data = Daily(point, start, end).fetch()\n",
    "            data = data[[\"tavg\", \"tmin\", \"tmax\", \"prcp\", \"wspd\"]]\n",
    "            data.to_csv(file_path)\n",
    "            print(f\"{name}: adat letöltve és mentve.\")\n",
    "        except Exception as e:\n",
    "            print(f\"{name} adatainak lekérése sikertelen: {e}\")\n",
    "            continue\n",
    "\n",
    "    data.columns = [f\"{name}_{col}\" for col in data.columns]\n",
    "    # Ellenőrizzük, hogy lefedi-e az időtartományt\n",
    "    if data.index.min() > start or data.index.max() < end:\n",
    "        print(f\"{name}: kihagyva, mert nem teljes az időtartomány ({data.index.min().date()} – {data.index.max().date()})\")\n",
    "        continue  # NE add hozzá az all_data-hoz\n",
    "\n",
    "    all_data[name] = data\n",
    "\n",
    "df = pd.concat(all_data.values(), axis=1, join=\"inner\")\n",
    "df.reset_index(inplace=True)\n",
    "df.to_csv(\"european_capitals_weather_combined.csv\", index=False)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CUDA elérhető:\", torch.cuda.is_available())\n",
    "print(\"CUDA verzió:\", torch.version.cuda)\n",
    "print(\"PyTorch build CUDA-támogatással:\", torch.backends.cudnn.enabled)\n",
    "print(\"Alapértelmezett eszköz:\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)  # shape: (n, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "n_steps = N_STEPS  # Hány napos visszatekintés legyen\n",
    "\n",
    "# Csak a feature-ök, a 'time' oszlop nélkül\n",
    "raw_features = df.drop(columns=[\"time\"])\n",
    "\n",
    "# Skálázás DataFrame-ként, hogy később könnyen eldobjuk az oszlopokat\n",
    "scaler = StandardScaler()\n",
    "scaled_features = pd.DataFrame(scaler.fit_transform(raw_features), columns=raw_features.columns)\n",
    "\n",
    "# NaN-t tartalmazó oszlopok eldobása\n",
    "cleaned_features = scaled_features.dropna(axis=1)\n",
    "\n",
    "# Numpy tömb formátumba visszaalakítás\n",
    "features = cleaned_features.values\n",
    "\n",
    "# Ellenőrzés\n",
    "print(\"Maradt oszlop:\", features.shape[1])\n",
    "\n",
    "# A cél: Budapest tmax\n",
    "target_column = TARGET_COLUMN\n",
    "target_series = df[target_column].values[n_steps:]\n",
    "\n",
    "# Ellenőrizzük, hogy a célváltozó létezik-e\n",
    "if target_column not in df.columns:\n",
    "    raise ValueError(f\"A megadott célváltozó ({target_column}) nem található a DataFrame-ben!\")\n",
    "\n",
    "# A cél: például \"Budapest_tmax\" — a teljes oszlopból, rolling window miatt eltolással\n",
    "target_series = df[target_column].values\n",
    "\n",
    "\n",
    "# Rolling window input generálás\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(n_steps, len(features)):\n",
    "    X.append(features[i - n_steps:i])\n",
    "    y.append(target_series[i])  \n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Célváltozók ellenőrzése - belefutottunk NaN értékekbe tanulítás során ezért megvizsgáljuk az adatokat\n",
    "print(\"Célváltozó (y) statisztika:\")\n",
    "print(\"Y NaN:\", np.isnan(y).any())\n",
    "print(\"Y Inf:\", np.isinf(y).any())\n",
    "print(\"Y max:\", np.max(y))\n",
    "print(\"Y min:\", np.min(y))\n",
    "print(f\"  Szórás: {np.std(y):.4f}\")\n",
    "print(f\"  Átlag: {np.mean(y):.4f}\")\n",
    "\n",
    "print(\"Célváltozó (x) statisztika:\")\n",
    "print(\"X NaN:\", np.isnan(X).any())\n",
    "print(\"X Inf:\", np.isinf(X).any())\n",
    "print(\"X max:\", np.max(X))\n",
    "print(\"X min:\", np.min(X))\n",
    "print(f\"  Szórás: {np.std(X):.4f}\")\n",
    "print(f\"  Átlag: {np.mean(X):.4f}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "train_dataset = WeatherDataset(X_train, y_train)\n",
    "test_dataset = WeatherDataset(X_test, y_test)\n",
    "\n",
    "# DataLoader-ek\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]  # csak az utolsó időlépés kimenetét használjuk\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=EARLY_STOPPING_PATIENCE, verbose=True, delta=1e-4):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping: {self.counter}/{self.patience} epoch óta nincs javulás.\", end=' ')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Modell inicializálás\n",
    "model = LSTMModel(input_size=X.shape[2], hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS).to(device)\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=EARLY_STOPPING_PATIENCE)\n",
    "epoch = 0\n",
    "train_loss_history = []\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "while epoch < 1000:  # maximum korlát, ha nincs early stop\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    print(f\"\\nEpoch {epoch+1}, Loss: {train_loss:.4f}\", end=' ')\n",
    "    train_loss_history.append(train_loss)\n",
    "\n",
    "\n",
    "    early_stopping(train_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Korai leállítás aktiválva.\")\n",
    "        break\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time  # másodpercben\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_suffix = f\"H{HIDDEN_SIZE}_L{NUM_LAYERS}_LR{LEARNING_RATE}_BS{BATCH_SIZE}_S{N_STEPS}_PAT{EARLY_STOPPING_PATIENCE}\"\n",
    "\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "\n",
    "csv_path = os.path.join(\"logs\", f\"train_loss_{fname_suffix}.csv\")\n",
    "png_path = os.path.join(\"plots\", f\"train_loss_{fname_suffix}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    \"epoch\": list(range(1, len(train_loss_history) + 1)),\n",
    "    \"train_loss\": train_loss_history\n",
    "}).to_csv(csv_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(train_loss_history, label=\"Train loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (MSE)\")\n",
    "plt.title(\"Tanulási görbe\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(png_path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Modell mentése ===\n",
    "MODEL_DIR = \"models\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(MODEL_DIR, f\"model_{fname_suffix}.pt\")\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "print(f\"Modell mentve: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs).cpu().numpy()\n",
    "        predictions.extend(outputs.flatten())\n",
    "        actuals.extend(targets.numpy().flatten())\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "mse = mean_squared_error(actuals, predictions)\n",
    "mae = mean_absolute_error(actuals, predictions)\n",
    "\n",
    "print(f\"Teszt MSE: {mse:.3f}\")\n",
    "print(f\"Teszt MAE: {mae:.3f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "log_file = \"training_log.csv\"\n",
    "log_columns = [\n",
    "    \"training\", \"hidden_size\", \"learning_rate\",\"layers\", \"epochs\", \"batch_size\",\n",
    "    \"train_loss\", \"test_mse\", \"test_mae\", \"elapsed_time\"\n",
    "]\n",
    "\n",
    "if not os.path.exists(log_file):\n",
    "    pd.DataFrame(columns=log_columns).to_csv(log_file, index=False)\n",
    "\n",
    "# Naplózás\n",
    "log_entry = {\n",
    "    \"training\": csv_path,\n",
    "    \"hidden_size\": HIDDEN_SIZE,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"layers\" : NUM_LAYERS,\n",
    "    \"epochs\" : epoch + 1,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"train_loss\": train_loss,\n",
    "    \"test_mse\": mse,\n",
    "    \"test_mae\": mae,\n",
    "    \"elapsed_time\": round(elapsed_time, 2)  # másodperc, 2 tizedesre kerekítve\n",
    "}\n",
    "\n",
    "# Napló hozzáfűzése\n",
    "log_df = pd.read_csv(log_file)\n",
    "log_df = pd.concat([log_df, pd.DataFrame([log_entry])], ignore_index=True)\n",
    "log_df.to_csv(log_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
