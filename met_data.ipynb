{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cél: Naponta bontott időjárási adatok letöltése több európai fővárosra 2020 és 2024 között.\n",
    "A letöltött adatokat egyesítjük, hogy egy közös DataFrame-ben legyenek.\n",
    "Ez szolgál majd bemenetként a mélytanuló modellhez, amely Budapest jövőbeli hőmérsékletét próbálja majd becsülni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install meteostat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modellparaméterek\n",
    "HIDDEN_SIZE = 32          # LSTM rejtett réteg mérete\n",
    "NUM_LAYERS = 1            # LSTM rétegek száma\n",
    "LEARNING_RATE = 0.001    # Tanulási ráta\n",
    "BATCH_SIZE = 32           # Batch méret\n",
    "N_STEPS = 7               # Rolling window méret (időlépések)\n",
    "EARLY_STOPPING_PATIENCE = 10\n",
    "TARGET_CITY = \"Budapest\"\n",
    "MAX_RADIUS_KM = 1000\n",
    "ONLY_CAPITALS = True\n",
    "\n",
    "# Célváltozó\n",
    "TARGET_COLUMN = \"Budapest_tmax\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# SimpleMaps CSV beolvasása\n",
    "cities_df = pd.read_csv(\"worldcities.csv\")  # Ha máshogy nevezted el, cseréld a fájlnevet\n",
    "\n",
    "# A célváros kiválasztása\n",
    "target_row = cities_df[cities_df[\"city_ascii\"] == TARGET_CITY]\n",
    "if target_row.empty:\n",
    "    raise ValueError(f\"A megadott célváros ({TARGET_CITY}) nem található az adatbázisban.\")\n",
    "\n",
    "target_latlon = (target_row.iloc[0][\"lat\"], target_row.iloc[0][\"lng\"])\n",
    "\n",
    "# Szűrés csak országfővárosokra, ha szükséges\n",
    "if ONLY_CAPITALS:\n",
    "    filtered_df = cities_df[cities_df[\"capital\"] == \"primary\"].copy()\n",
    "else:\n",
    "    filtered_df = cities_df.copy()\n",
    "\n",
    "# Távolság szerinti szűrés\n",
    "def within_radius(row):\n",
    "    return geodesic(target_latlon, (row[\"lat\"], row[\"lng\"])).km <= MAX_RADIUS_KM\n",
    "\n",
    "filtered_df = filtered_df[filtered_df.apply(within_radius, axis=1)]\n",
    "\n",
    "# Célvárost biztosan hozzáadjuk, ha még nem szerepel\n",
    "if TARGET_CITY not in filtered_df[\"city_ascii\"].values:\n",
    "    filtered_df = pd.concat([filtered_df, target_row])\n",
    "\n",
    "# Koordinátaszótár előállítása\n",
    "city_coordinates = {\n",
    "    row[\"city_ascii\"]: (row[\"lat\"], row[\"lng\"]) for _, row in filtered_df.iterrows()\n",
    "}\n",
    "\n",
    "print(f\"{len(city_coordinates)} város kiválasztva {MAX_RADIUS_KM} km-en belül {TARGET_CITY} körül.\")\n",
    "print(\" ->\", \", \".join(sorted(city_coordinates.keys())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "import time\n",
    "\n",
    "# Használjuk az előzőleg előállított városlistát (ne a fix 'cities' listát)\n",
    "geolocator = Nominatim(user_agent=\"weather_project\")\n",
    "verified_coordinates = {}\n",
    "\n",
    "for city in city_coordinates.keys():\n",
    "    try:\n",
    "        location = geolocator.geocode(city + \", Europe\")\n",
    "        if location:\n",
    "            lat, lon = location.latitude, location.longitude\n",
    "            verified_coordinates[city] = (lat, lon)\n",
    "            print(f\"{city}: ({lat:.4f}, {lon:.4f})\")\n",
    "        else:\n",
    "            print(f\"{city}: Nem található.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Hiba {city} lekérdezésekor: {e}\")\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "# Ha ezt a validált listát használnád később:\n",
    "city_coordinates = verified_coordinates\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from meteostat import Point, Daily\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Időtartomány megadása: 2020. január 1. – 2024. december 31.\n",
    "start = datetime(2000, 1, 1)\n",
    "end = datetime(2024, 12, 31)\n",
    "\n",
    "# Üres szótár az egyes városok adatainak tárolásához\n",
    "all_data = {}\n",
    "\n",
    "# Adatok lekérése minden városra külön\n",
    "for name, (lat, lon) in city_coordinates.items():\n",
    "    try:\n",
    "        # Meteostat pozíció objektum létrehozása\n",
    "        point = Point(lat, lon)\n",
    "        \n",
    "        # Napi időjárási adatok lekérése\n",
    "        data = Daily(point, start, end).fetch()\n",
    "        \n",
    "        # Csak a szükséges oszlopokat tartjuk meg\n",
    "        data = data[[\"tavg\", \"tmin\", \"tmax\", \"prcp\", \"wspd\"]]\n",
    "        \n",
    "        # Oszlopok átnevezése a városnév hozzáadásával\n",
    "        data.columns = [f\"{name}_tavg\", f\"{name}_tmin\", f\"{name}_tmax\", f\"{name}_prcp\", f\"{name}_wspd\"]\n",
    "        \n",
    "        # Adatok eltárolása a szótárban\n",
    "        all_data[name] = data\n",
    "        print(f\"{name} adatai sikeresen letöltve.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"{name} adatainak lekérése sikertelen: {e}\")\n",
    "\n",
    "# Az összes város adatainak egyesítése közös időindex mentén\n",
    "# Csak azok a napok maradnak meg, amelyek minden városnál rendelkezésre állnak\n",
    "df = pd.concat(all_data.values(), axis=1, join='inner')\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "# Az eredmény exportálása CSV formátumban (opcionális)\n",
    "df.to_csv(\"european_capitals_weather.csv\", index=False)\n",
    "\n",
    "# Az első néhány sor megjelenítése ellenőrzés céljából\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CUDA elérhető:\", torch.cuda.is_available())\n",
    "print(\"CUDA verzió:\", torch.version.cuda)\n",
    "print(\"PyTorch build CUDA-támogatással:\", torch.backends.cudnn.enabled)\n",
    "print(\"Alapértelmezett eszköz:\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)  # shape: (n, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "n_steps = N_STEPS  # Hány napos visszatekintés legyen\n",
    "\n",
    "# Csak a feature-ök, a 'time' oszlop nélkül\n",
    "raw_features = df.drop(columns=[\"time\"])\n",
    "\n",
    "# Skálázás DataFrame-ként, hogy később könnyen eldobjuk az oszlopokat\n",
    "scaler = StandardScaler()\n",
    "scaled_features = pd.DataFrame(scaler.fit_transform(raw_features), columns=raw_features.columns)\n",
    "\n",
    "# NaN-t tartalmazó oszlopok eldobása\n",
    "cleaned_features = scaled_features.dropna(axis=1)\n",
    "\n",
    "# Numpy tömb formátumba visszaalakítás\n",
    "features = cleaned_features.values\n",
    "\n",
    "# Ellenőrzés\n",
    "print(\"Maradt oszlop:\", features.shape[1])\n",
    "\n",
    "# A cél: Budapest tmax\n",
    "target_column = TARGET_COLUMN\n",
    "target_series = df[target_column].values[n_steps:]\n",
    "\n",
    "# Ellenőrizzük, hogy a célváltozó létezik-e\n",
    "if target_column not in df.columns:\n",
    "    raise ValueError(f\"A megadott célváltozó ({target_column}) nem található a DataFrame-ben!\")\n",
    "\n",
    "# A cél: például \"Budapest_tmax\" — a teljes oszlopból, rolling window miatt eltolással\n",
    "target_series = df[target_column].values\n",
    "\n",
    "\n",
    "# Rolling window input generálás\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(n_steps, len(features)):\n",
    "    X.append(features[i - n_steps:i])\n",
    "    y.append(target_series[i])  \n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Célváltozók ellenőrzése - belefutottunk NaN értékekbe tanulítás során ezért megvizsgáljuk az adatokat\n",
    "print(\"Célváltozó (y) statisztika:\")\n",
    "print(\"Y NaN:\", np.isnan(y).any())\n",
    "print(\"Y Inf:\", np.isinf(y).any())\n",
    "print(\"Y max:\", np.max(y))\n",
    "print(\"Y min:\", np.min(y))\n",
    "print(f\"  Szórás: {np.std(y):.4f}\")\n",
    "print(f\"  Átlag: {np.mean(y):.4f}\")\n",
    "\n",
    "print(\"Célváltozó (x) statisztika:\")\n",
    "print(\"X NaN:\", np.isnan(X).any())\n",
    "print(\"X Inf:\", np.isinf(X).any())\n",
    "print(\"X max:\", np.max(X))\n",
    "print(\"X min:\", np.min(X))\n",
    "print(f\"  Szórás: {np.std(X):.4f}\")\n",
    "print(f\"  Átlag: {np.mean(X):.4f}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "train_dataset = WeatherDataset(X_train, y_train)\n",
    "test_dataset = WeatherDataset(X_test, y_test)\n",
    "\n",
    "# DataLoader-ek\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]  # csak az utolsó időlépés kimenetét használjuk\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=EARLY_STOPPING_PATIENCE, verbose=True, delta=1e-4):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping: {self.counter}/{self.patience} epoch óta nincs javulás.\", end=' ')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Modell inicializálás\n",
    "model = LSTMModel(input_size=X.shape[2], hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS).to(device)\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=10)\n",
    "epoch = 0\n",
    "train_loss_history = []\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "while epoch < 1000:  # maximum korlát, ha nincs early stop\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    print(f\"\\nEpoch {epoch+1}, Loss: {train_loss:.4f}\", end=' ')\n",
    "    train_loss_history.append(train_loss)\n",
    "\n",
    "\n",
    "    early_stopping(train_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Korai leállítás aktiválva.\")\n",
    "        break\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time  # másodpercben\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_suffix = f\"H{HIDDEN_SIZE}_L{NUM_LAYERS}_LR{LEARNING_RATE}_BS{BATCH_SIZE}_S{N_STEPS}_PAT{EARLY_STOPPING_PATIENCE}\"\n",
    "\n",
    "csv_path = f\"train_loss_{fname_suffix}.csv\"\n",
    "png_path = f\"train_loss_{fname_suffix}.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    \"epoch\": list(range(1, len(train_loss_history) + 1)),\n",
    "    \"train_loss\": train_loss_history\n",
    "}).to_csv(csv_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(train_loss_history, label=\"Train loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (MSE)\")\n",
    "plt.title(\"Tanulási görbe\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(png_path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs).cpu().numpy()\n",
    "        predictions.extend(outputs.flatten())\n",
    "        actuals.extend(targets.numpy().flatten())\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "mse = mean_squared_error(actuals, predictions)\n",
    "mae = mean_absolute_error(actuals, predictions)\n",
    "\n",
    "print(f\"Teszt MSE: {mse:.3f}\")\n",
    "print(f\"Teszt MAE: {mae:.3f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "log_file = \"training_log.csv\"\n",
    "log_columns = [\n",
    "    \"training\", \"hidden_size\", \"learning_rate\",\"layers\", \"epochs\", \"batch_size\",\n",
    "    \"train_loss\", \"test_mse\", \"test_mae\", \"elapsed_time\"\n",
    "]\n",
    "\n",
    "if not os.path.exists(log_file):\n",
    "    pd.DataFrame(columns=log_columns).to_csv(log_file, index=False)\n",
    "\n",
    "# Naplózás\n",
    "log_entry = {\n",
    "    \"training\": csv_path,\n",
    "    \"hidden_size\": HIDDEN_SIZE,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"layers\" : NUM_LAYERS,\n",
    "    \"epochs\" : epoch + 1,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"train_loss\": train_loss,\n",
    "    \"test_mse\": mse,\n",
    "    \"test_mae\": mae,\n",
    "    \"elapsed_time\": round(elapsed_time, 2)  # másodperc, 2 tizedesre kerekítve\n",
    "}\n",
    "\n",
    "# Napló hozzáfűzése\n",
    "log_df = pd.read_csv(log_file)\n",
    "log_df = pd.concat([log_df, pd.DataFrame([log_entry])], ignore_index=True)\n",
    "log_df.to_csv(log_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
